# Latency vs Throughput

Latency and throughput are two core metrics for measuring the performance of networks, applications, and distributed systems.  
They are **related but different**: latency measures **how long** a single operation takes, while throughput measures **how much** work is completed over time.

---

## Definitions

- **Latency**  
  The total time it takes for a single unit of data (or a request) to travel from the **source** to the **destination** and receive a response.  
  - Often called *response time* or *delay*.
  - Includes all processing, queuing, and transmission delays.

- **Throughput**  
  The amount of data or number of operations a system can process in a **given period of time**.  
  - Indicates the *capacity* or *bandwidth* of a system.
  - Typically expressed as requests/second, transactions/second, or bits/second (bps).

---

## Key Characteristics

| **Aspect**         | **Latency**                                                    | **Throughput**                                                       |
|--------------------|----------------------------------------------------------------|-----------------------------------------------------------------------|
| **Meaning**        | Time for a single operation to complete.                       | Volume of operations or data processed per unit of time.              |
| **Unit**           | Milliseconds (ms), microseconds (µs).                           | Requests/second, MB/s, Gbps, etc.                                    |
| **Focus**          | Speed of *one* interaction.                                     | Total capacity of the system.                                        |
| **Goal**           | Reduce delay per request.                                       | Maximize total work done.                                            |
| **Analogy**        | Time for one car to travel from city A to B.                    | Number of cars that can pass a highway per minute.                    |

---

## Components of Latency

Latency is not just network delay—it is the sum of multiple factors:

1. **Propagation Delay** – Time for a signal to travel across the medium (limited by the speed of light in fiber/copper).

2. **Transmission Delay** – Time to push all packet bits onto the wire (depends on link bandwidth).

3. **Processing Delay** – Time routers/servers take to examine headers and forward packets.

4. **Queuing Delay** – Time spent waiting in router or server queues when traffic is congested.

5. **Application Delay** – Time spent in server-side processing (database queries, business logic).

Total Latency = Propagation + Transmission + Processing + Queuing + Application

---

## Factors Affecting Throughput

- **Bandwidth**: Maximum capacity of the network link.

- **Concurrency**: Ability to handle multiple parallel connections or threads.

- **Protocol Efficiency**: Overheads of TCP, HTTP, or encryption.

- **Server Resources**: CPU, memory, disk I/O limits.

- **Congestion & Packet Loss**: Retransmissions reduce effective throughput.

---

## Relationship Between Latency & Throughput

- They are **related but not directly proportional**.  
  - Low latency doesn’t guarantee high throughput (e.g., a fast but narrow channel).  
  - High throughput doesn’t guarantee low latency (e.g., a wide channel with heavy queuing delays).

**Example**:  
- A 1 Gbps link with 200 ms latency can transfer large files quickly (high throughput) but individual small requests feel slow.  
- A 10 ms link with only 1 Mbps bandwidth responds quickly but can’t handle large file transfers well.

---

## Optimization Techniques

### Reducing Latency

- **Edge Computing / CDNs**: Bring content closer to users.

- **Caching**: Reduce time to fetch repeated data.

- **Efficient Protocols**: Use HTTP/2, QUIC, or gRPC for faster handshakes and multiplexing.

- **Faster Hardware**: SSDs, faster CPUs, and optimized code paths.

### Increasing Throughput

- **Horizontal Scaling**: Add more servers to process requests in parallel.

- **Load Balancing**: Distribute traffic evenly.

- **Batching & Compression**: Send larger chunks of data at once, reduce packet overhead.

- **High-capacity Links**: Upgrade network bandwidth.

---

## When to Prioritize Each

| **Application Type**               | **Priority**                          |
|------------------------------------|----------------------------------------|
| Real-time gaming, video calls       | **Low Latency** is critical            |
| File transfer, video streaming      | **High Throughput** is key             |
| Financial trading systems           | Extremely low latency is mandatory     |
| Data pipelines, backups             | Max throughput for large data volumes  |

---