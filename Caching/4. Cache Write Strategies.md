# Cache Write Policies

## Definition

**Cache write policies** define **how and when data is written to the underlying database or storage** when a cache is used.

When an application updates data, the change may be written:

* directly to the database
* first to the cache
* to both cache and database in different orders
* immediately or asynchronously

The chosen policy affects:

* data consistency
* durability
* performance
* risk of data loss

---

## Why Write Policies Are Important

A cache sits **between application and database**.
When data is updated, the system must decide:

* Should the **cache be updated**?
* Should the **database be updated**?
* In which order?
* Immediately or later?
* What happens on failures?

The wrong policy may cause:

* stale data
* inconsistent reads
* lost writes
* heavy database load

The right policy balances:

* performance
* correctness
* fault tolerance

---

## 1. Write-Through Cache

### Idea

In **write-through caching**, data is written to:

1. cache
2. database

**synchronously**, in the same operation.

The write is considered successful **only after both** cache and database are updated.

### How it works

* application writes data
* cache updates the value
* cache immediately writes the same value to database
* acknowledgment returned to application

### Advantages

* strong consistency between cache and database
* cache always has fresh data
* easy to reason about correctness

### Disadvantages

* slower writes because database is in the critical path
* higher write amplification
* increased write load on database

### When to use

* financial systems
* inventory management
* systems where **consistency is more important than write latency**

---

## 2. Write-Back (Write-Behind) Cache

### Idea

In **write-back caching**, data is **first written to cache**, and the cache updates the database **asynchronously later**.

The database is updated in the background after some delay or batching.

### How it works

* application writes to cache
* cache acknowledges success immediately
* data is later flushed to database asynchronously

### Advantages

* very fast write latency
* database load reduced by batching writes
* ideal for high-write workloads

### Disadvantages

* data loss risk if cache node fails before flushing
* temporary inconsistency between cache and database
* more complex implementation

### When to use

* analytics systems
* logging and metrics
* systems tolerant to slight consistency delay

---

## 3. Write-Around Cache

### Idea

In **write-around caching**, writes bypass the cache entirely and are written **only to database**.

The cache is **not updated during writes**.

Data enters the cache **only when read later**.

### How it works

* application writes directly to database
* cache remains unchanged
* if data is later requested → cache miss → fetch from DB → populate cache

### Advantages

* avoids polluting cache with rarely read data
* reduces unnecessary cache writes
* useful for write-heavy but read-light workloads

### Disadvantages

* higher read latency immediately after writes
* cache misses right after update are common

### When to use

* workloads where new data is rarely reread
* large write-heavy IoT/event streams

---

## 4. Read-Through Cache

### Idea

In **read-through caching**, application reads **only from cache**.
If data is missing, cache itself loads it from database and stores it.

The application never talks to database directly.

### How it works

* application requests data from cache
* if present → returned immediately
* if absent → cache fetches from DB → stores → returns to app

### Advantages

* simplified application logic
* automatic cache population
* consistent caching behavior

### Disadvantages

* cache layer becomes more complex
* higher latency on first read (cache miss)

### When to use

* microservices architectures
* systems with predictable read patterns

---

## 5. Write-Through vs Write-Back vs Write-Around

| Policy            | Writes To DB | Consistency          | Write Speed | Risk of Data Loss | Typical Use Case      |
| ----------------- | ------------ | -------------------- | ----------- | ----------------- | --------------------- |
| **Write-Through** | Immediately  | Strong (cache = DB)  | Slower      | Very low          | Banking, payments     |
| **Write-Back**    | Later        | Eventual consistency | Very fast   | Possible on crash | Analytics, logs       |
| **Write-Around**  | Immediately  | Cache may be stale   | Moderate    | Low               | Write-heavy workloads |

---

## Write Allocate vs No-Write Allocate (Optional)

These terms define **what happens to cache on a write miss**.

### Write-Allocate

On write miss:

* block is loaded into cache
* write happens in cache

Used with:

* write-back caches

### No-Write-Allocate

On write miss:

* data written directly to database
* cache not updated

Used with:

* write-around policies

---

## Which Policy Should You Choose?

* Choose **write-through** when data correctness is critical
* Choose **write-back** when performance is highest priority
* Choose **write-around** for write-heavy workloads with low rereads
* Combine read-through with any of the above for simplicity

**Real systems often mix strategies**:

* read-through + write-through
* read-through + write-back
* cache-aside + write-around

---

## Summary

* Cache write policies define **how updates propagate between cache and database**.

* **Write-through** ensures consistency but increases write latency

* **Write-back** gives best performance but risks temporary data loss

* **Write-around** reduces cache pollution but increases read misses

* **Read-through** centralizes data access through cache

---
