# Introduction to Caching

## Definition

* **Caching** is the process of **storing frequently accessed data in a temporary storage layer (cache)** so that future requests for that data can be served **faster**, without repeatedly querying the underlying database or service.

* It helps improve **performance, scalability, and response time** in distributed systems by **reducing database load** and **network latency**.

---

## Why Caching is Needed

* To **reduce latency** and deliver faster responses.

* To **minimize database load** by avoiding redundant queries.

* To **handle high traffic efficiently** without scaling database infrastructure.

* To **improve user experience** with quicker access to frequently used data.

* To **reduce costs** by minimizing expensive compute and I/O operations.

---

## How Caching Works

![Caching Diagram](images/cache_working.png)

1. **Client Request:**
   The client requests data (e.g., user profile or search result).

2. **Cache Lookup:**
   The system first checks the cache (in-memory store like Redis).

   * If **data is found** → returns immediately (*cache hit*).
   * If **not found** → retrieves from the database (*cache miss*).

3. **Database Query:**
   On a cache miss, the application queries the database or external API.

4. **Cache Update:**
   The retrieved data is stored in the cache for future use.

5. **Response to Client:**
   The data is sent back to the user, typically in **milliseconds** from the cache.

---

## Key Concepts in Caching

| **Term**               | **Definition**                                                                    |
| ---------------------- | --------------------------------------------------------------------------------- |
| **Cache Hit**          | When requested data is found in the cache.                                        |
| **Cache Miss**         | When data is not found in the cache and must be fetched from the original source. |
| **Cache Eviction**     | The process of removing data from the cache when it’s full or expired.            |
| **TTL (Time-To-Live)** | Duration for which data remains valid in the cache before it expires.             |
| **Warm Cache**         | Cache that already contains frequently used data.                                 |
| **Cold Cache**         | Cache that starts empty after initialization or flush.                            |

---

## Types of Caching

### 1. Client-Side Caching

* Data is cached at the **user’s device** or **browser**.

* Example: Browser caching static assets (CSS, JS, images).

* **Pros:** Reduces server requests.

* **Cons:** Limited to individual clients, less control.

---

### 2. Server-Side Caching

* Data is cached on the **application server** or **dedicated cache server** (e.g., Redis, Memcached).

* Example: API response caching, session caching.

* **Pros:** Centralized and scalable.

* **Cons:** Requires cache management and invalidation logic.

---

### 3. CDN Caching (Edge Caching)

* Content cached on **CDN edge servers** near end users.* Used for static assets like images, videos, and files.

* Example: Cloudflare, Akamai, AWS CloudFront.

* **Pros:** Reduces latency for global users.

* **Cons:** Limited for dynamic or frequently changing data.

---

### 4. Database Caching

* Frequently queried results stored in memory or on dedicated caching layers.

* Example: Caching query results or computed aggregates in Redis.

* **Pros:** Offloads database queries.

* **Cons:** Risk of stale data if not updated correctly.

---

## Benefits of Caching

| **Benefit**                  | **Explanation**                                            |
| ---------------------------- | ---------------------------------------------------------- |
| **Reduced Latency**          | Serves data from memory instead of slower disk or network. |
| **Increased Throughput**     | Handles more requests per second with the same resources.  |
| **Lower Database Load**      | Minimizes repetitive queries.                              |
| **Cost Efficiency**          | Reduces infrastructure and compute usage.                  |
| **Improved User Experience** | Faster responses and smoother interactions.                |

---

## Challenges of Caching

| **Challenge**          | **Description**                                                      |
| ---------------------- | -------------------------------------------------------------------- |
| **Cache Invalidation** | Ensuring cached data stays consistent with the source.               |
| **Stale Data**         | Cached data can become outdated if not refreshed.                    |
| **Eviction Policies**  | Deciding which data to remove when the cache is full.                |
| **Cache Stampede**     | Multiple requests for the same missing key can overload the backend. |
| **Data Consistency**   | Harder to maintain in distributed systems.                           |

---

## Cache Strategies

| **Strategy**                   | **Description**                                                                                      |
| ------------------------------ | ---------------------------------------------------------------------------------------------------- |
| **Read-Through Cache**         | Application reads from cache; if miss, fetches from DB and updates cache automatically.              |
| **Write-Through Cache**        | Data is written to cache and database simultaneously.                                                |
| **Write-Behind (Write-Back)**  | Data first written to cache, then asynchronously written to DB.                                      |
| **Cache-Aside (Lazy Loading)** | Application explicitly checks cache first; on miss, loads from DB and updates cache. *(Most common)* |

---

## Common Cache Systems

| **Technology**               | **Type**                    | **Use Case**                               |
| ---------------------------- | --------------------------- | ------------------------------------------ |
| **Redis**                    | In-memory key-value store   | General-purpose caching, pub/sub, sessions |
| **Memcached**                | In-memory distributed cache | High-speed simple caching                  |
| **CDN (CloudFront, Akamai)** | Edge cache                  | Static content distribution                |
| **Varnish**                  | HTTP accelerator            | Web content caching                        |

---

## Best Practices

* Choose appropriate **TTL** based on data volatility.

* Implement **cache invalidation policies** (`write-through`, `cache-aside`, etc.).

* Use **LRU/LFU eviction policies** to manage memory efficiently.

* Monitor **cache hit ratio** for effectiveness.

* Avoid caching **sensitive or user-specific** data unless encrypted.

* Combine caching with **load balancing** and **replication** for high availability.

---

## In Summary

| **Aspect**          | **Without Cache** | **With Cache**        |
| ------------------- | ----------------- | --------------------- |
| **Response Time**   | High (DB lookup)  | Low (in-memory fetch) |
| **Database Load**   | Heavy             | Light                 |
| **Scalability**     | Limited           | High                  |
| **User Experience** | Slower            | Faster                |

> Caching is one of the **most powerful techniques** to scale systems —
> it trades **memory and freshness** for **speed and performance**.

---