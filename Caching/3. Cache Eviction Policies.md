# Cache Eviction Policies

## Definition

A **cache eviction policy** is the strategy used to decide **which data items should be removed from the cache** when the cache becomes full, when data becomes stale, or when higher-priority data needs to be stored.

Because cache memory is usually **fast but limited (mostly RAM)**, it cannot store everything indefinitely. Eviction policies ensure that the cache continues to hold the **most relevant and useful data**, while less useful or outdated data is discarded.

---

## Why Eviction Policies Are Needed

Cache improves performance only when it stores **useful, frequently accessed data**.
Without an eviction policy:

* cache would fill up permanently
* new data could not be stored
* stale entries would stay forever
* hit ratio would drop
* memory could be wasted

Eviction policies help decide:

* **what should remain in cache**
* **what should be removed**
* **how to handle limited memory**
* **how to prioritize hot vs cold data**

---

## Important Basic Terms

### Cache Hit

Requested data is found in cache → returned quickly.

### Cache Miss

Requested data not in cache → fetched from slower database.

### Hot Data

Frequently accessed items likely to be used again.

### Cold Data

Rarely accessed items that can be removed safely.

### TTL (Time-to-Live)

Configured lifetime after which a cached item automatically expires.

---

## 1. LRU (Least Recently Used)

### Idea

LRU removes the item that has **not been accessed for the longest time**.
It assumes:

> “Data not used recently is less likely to be used soon.”

### How it works (simple view)

* tracks recent access order
* when full → discards the **least recently accessed** item

### Where it is used

* Redis (very common)
* Operating systems page replacement
* Web application caches

### Pros

* strong practical performance in most systems
* intuitive and widely supported

### Cons

* small overhead to track recency
* Can perform poorly when workloads scan large datasets once

---

## 2. LFU (Least Frequently Used)

### **Idea**

LFU removes the item that has been **accessed the least number of times** overall.

> “If data is rarely used, it does not deserve cache space.”

### Mechanism

* maintains a **frequency counter** for every cached key
* evicts entries with **lowest access count**

### Strengths

* preserves long-term “hot” data
* good for workloads with stable popularity patterns

### Weaknesses

* costlier to maintain counters
* new items may be evicted too soon due to low initial frequency

---

## 3. FIFO (First In, First Out)

### Idea

Evict the **oldest inserted item first**, purely based on arrival time.

> “First to come, first to leave.”

### Pros

* extremely simple
* low bookkeeping overhead

### Cons

* ignores recency and frequency
* highly used items can get evicted accidentally

---

## 4. MRU (Most Recently Used)

### Idea

Evict the **most recently accessed item** first — the opposite of LRU.

Useful when recent data is **unlikely** to be reused, for example:

* sequential scans
* batch processing

### Limitation

* usually performs poorly for typical web workloads

---

## 5. Random Replacement

### Idea

Evict a **randomly chosen item**, without tracking frequency or recency.

### When it is used

* ultra-high-speed systems
* extremely limited devices
* when tracking metadata is expensive

### Trade-off

* very easy to implement
* unpredictable hit ratio

---

## 6. TTL-Based Eviction

### Idea

Every item has an **expiry time**, and once that time is reached, it is automatically removed regardless of usage.

Typical examples:

* authentication tokens
* DNS records
* short-lived cache entries

### Strengths

* guarantees freshness
* avoids stale data buildup

### Weaknesses

* popular data may expire unnecessarily

---

## Policy Comparison Table

| Policy     | Main Principle               | Best For                   | Drawback               |
| ---------- | ---------------------------- | -------------------------- | ---------------------- |
| **LRU**    | Remove least recently used   | General web systems, Redis | Needs recency tracking |
| **LFU**    | Remove least frequently used | Long-term popularity       | Counter overhead       |
| **FIFO**   | Remove oldest inserted       | Simple hardware, queues    | Ignores usage pattern  |
| **MRU**    | Remove most recent           | Sequential scans           | Rarely optimal         |
| **Random** | Remove random entry          | Very fast caches           | Unpredictable results  |
| **TTL**    | Expire after time            | Freshness is critical      | Wastes hot entries     |

---

## Which Policy Should You Choose?

* **LRU** → best default choice, works for most systems

* **LFU** → when popularity remains stable for long time

* **FIFO** → when simplicity matters more than accuracy

* **TTL** → when freshness matters more than hit-ratio

* **MRU** → only when access pattern is sequential or scan-heavy

---

## Summary

- Cache eviction policies are essential for managing limited cache memory effectively.

- Different policies suit different workloads and access patterns.

- **LRU** is the most commonly used and generally effective policy.

- **LFU** is beneficial for stable popularity scenarios.

- **FIFO and Random** are simpler but less effective in most cases.

- **TTL-based** eviction is crucial when data freshness is a priority.

---

